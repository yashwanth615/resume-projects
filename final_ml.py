"""final_ml.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_69YsGHSuy4Ke6fUXLPLVdAu3J6NlxoZ
"""

# Step 1: Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# prompt: code to extract zip file from drive
!unzip /content/drive/MyDrive/NutritionDeficientDataset.zip -d /content/drive/MyDrive/extracted_files

train_dir = '/content/drive/MyDrive/ml_proj/new_ds'
# !ls /content/drive/MyDrive

# # Step 3: Extract CNN Features and Save to CSV
# import os
# import pandas as pd
# from tensorflow.keras.applications import ResNet50
# from tensorflow.keras.applications.resnet50 import preprocess_input
# from tensorflow.keras.preprocessing.image import ImageDataGenerator
# from tensorflow.keras.models import Model

# # Model setup
# image_size = (224, 224)
# batch_size = 32
# base_model = ResNet50(weights='imagenet', include_top=False, pooling='avg')
# model = Model(inputs=base_model.input, outputs=base_model.output)

# # Image loader
# datagen = ImageDataGenerator(preprocessing_function=preprocess_input)
# train_gen = datagen.flow_from_directory(
#     train_dir,
#     target_size=image_size,
#     batch_size=batch_size,
#     class_mode='sparse',
#     shuffle=False
# )

# # Feature extraction
# features = model.predict(train_gen, verbose=1)
# labels = train_gen.classes

# # Save features to CSV
# df = pd.DataFrame(features)
# df['label'] = labels
# df.to_csv('/content/train_features.csv', index=False)

# print(" Features extracted and saved to: /content/train_features.csv")

'''
import zipfile
import os

zip_path = '/content/drive/MyDrive/train/deficiency-20250422T183912Z-001.zip'  # replace with actual filename
extract_to = '/content/drive/MyDrive/train/dunzipped_data'

# Create destination folder if it doesn't exist
os.makedirs(extract_to, exist_ok=True)

# Unzip the file
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_to)

print(" Zip file extracted!")
'''



# Step 3: Extract CNN Features and Save to CSV
import os
import pandas as pd
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.applications.resnet50 import preprocess_input
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Model

# Model setup
image_size = (224, 224)
batch_size = 32
base_model = ResNet50(weights='imagenet', include_top=False, pooling='avg')
model = Model(inputs=base_model.input, outputs=base_model.output)

# Image loader
datagen = ImageDataGenerator(preprocessing_function=preprocess_input)
train_gen = datagen.flow_from_directory(
    train_dir,
    target_size=image_size,
    batch_size=batch_size,
    class_mode='sparse',
    shuffle=False
)

# Feature extraction
features = model.predict(train_gen, verbose=1)
labels = train_gen.classes

# Save features to CSV
df = pd.DataFrame(features)
df['label'] = labels
df.to_csv('/content/train_features.csv', index=False)

print(" Features extracted and saved to: /content/train_features.csv")


import pandas as pd

# Load the CSV file
df = pd.read_csv('/content/drive/MyDrive/ml_proj/train_features.csv')

# Replace values in the 'label' column
df['label'] = df['label'].replace({1: 0, 2: 1})

# Save the updated CSV file
df.to_csv('/content/train_features.csv', index=False)

'''
from google.colab import files
files.download('/content/train_features.csv')
'''

'''
import pandas as pd
from sklearn.feature_selection import SelectKBest, f_classif

# Load your feature CSV file
df = pd.read_csv('train_features.csv')

# Remove rows where the label is NaN
df_clean = df.dropna(subset=['label'])

# Split features and labels
X_clean = df_clean.drop('label', axis=1).values
y_clean = df_clean['label'].values

# Select top 300 features based on ANOVA F-score
selector = SelectKBest(score_func=f_classif, k=300)
X_selected = selector.fit_transform(X_clean, y_clean)

# Save selected features to a new CSV
df_selected = pd.DataFrame(X_selected)
df_selected['label'] = y_clean
df_selected.to_csv('/content/train_features_top300.csv', index=False)

print(" Top 300 features selected and saved to: /content/train_features_top300.csv")
'''
import pandas as pd
import numpy as np
from sklearn.feature_selection import f_classif

# Load your feature CSV
df = pd.read_csv('train_features.csv')
df = df.dropna(subset=['label'])

# Separate features and labels
X = df.drop('label', axis=1)
y = df['label']

# Split by class
X_class0 = X[y == 0]
X_class1 = X[y == 1]

# Create fake labels for scoring (1 for this class, 0 for the rest)
fake_y0 = np.ones(X_class0.shape[0])
fake_y1 = np.ones(X_class1.shape[0])

# F-score (ANOVA) for each class separately
F0, _ = f_classif(X_class0, fake_y0)
F1, _ = f_classif(X_class1, fake_y1)

# Get indices of top 150 features for each class
top_150_class0 = np.argsort(F0)[-150:]
top_150_class1 = np.argsort(F1)[-150:]

# Combine and deduplicate
combined_indices = np.unique(np.concatenate([top_150_class0, top_150_class1]))
print(f" Selected {len(combined_indices)} unique features (balanced from both classes)")

# Subset the features using combined indices
X_selected = X.iloc[:, combined_indices]
X_selected['label'] = y.values

# Save to CSV
X_selected.to_csv('/content/train_features_balanced_top300.csv', index=False)
print(" Balanced top features saved to: /content/train_features_balanced_top300.csv")

import pandas as pd
import xgboost as xgb
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

# --------------------------
# Top 300 Features
# --------------------------
df_300 = pd.read_csv('/content/drive/MyDrive/ml_proj/train_features_top300.csv')
X_300 = df_300.drop('label', axis=1).values
y_300 = df_300['label'].values

# Impute missing values in y (labels) with the most frequent value (mode)
imputer_y_300 = SimpleImputer(strategy='most_frequent')
y_300_imputed = imputer_y_300.fit_transform(y_300.reshape(-1, 1)).flatten()

# Optional: Standardize
scaler_300 = StandardScaler()
X_300_scaled = scaler_300.fit_transform(X_300)

# XGBoost Classifier
xgb_300 = xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')
scores_300 = cross_val_score(xgb_300, X_300_scaled, y_300_imputed, cv=5, scoring='accuracy')
print(" Accuracy with top 300 features:", round(scores_300.mean(), 4))

# --------------------------
# All 2048 Features
# --------------------------
df_full = pd.read_csv('train_features.csv')
X_full = df_full.drop('label', axis=1).values
y_full = df_full['label'].values

# Impute missing values in y (labels) with the most frequent value (mode)
imputer_y_full = SimpleImputer(strategy='most_frequent')
y_full_imputed = imputer_y_full.fit_transform(y_full.reshape(-1, 1)).flatten()

# Standardize
scaler_full = StandardScaler()
X_full_scaled = scaler_full.fit_transform(X_full)

# XGBoost Classifier
xgb_full = xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')
scores_full = cross_val_score(xgb_full, X_full_scaled, y_full_imputed, cv=5, scoring='accuracy')
print("ðŸ” Accuracy with all 2048 features:", round(scores_full.mean(), 4))

#for train we repeat the same process
train_dir = '/content/drive/MyDrive/extracted_files/NutritionDeficientDataset/test'

# Step 3: Extract CNN Features and Save to CSV
import os
import pandas as pd
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.applications.resnet50 import preprocess_input
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Model

# Model setup
image_size = (224, 224)
batch_size = 32
base_model = ResNet50(weights='imagenet', include_top=False, pooling='avg')
model = Model(inputs=base_model.input, outputs=base_model.output)

# Image loader
datagen = ImageDataGenerator(preprocessing_function=preprocess_input)
train_gen = datagen.flow_from_directory(
    train_dir,
    target_size=image_size,
    batch_size=batch_size,
    class_mode='sparse',
    shuffle=False
)

# Feature extraction
features = model.predict(train_gen, verbose=1)
labels = train_gen.classes

# Save features to CSV
df = pd.DataFrame(features)
df['label'] = labels
df.to_csv('/content/test_features.csv', index=False)

print(" Features extracted and saved to: /content/test_features.csv")

import pandas as pd
import numpy as np
from sklearn.feature_selection import f_classif

# Load your feature CSV
df = pd.read_csv('/content/drive/MyDrive/ml_proj/train_features.csv')
df = df.dropna(subset=['label'])

# Separate features and labels
X = df.drop('label', axis=1)
y = df['label']

# Split by class
X_class0 = X[y == 0]
X_class1 = X[y == 1]

# Create fake labels for scoring (1 for this class, 0 for the rest)
fake_y0 = np.ones(X_class0.shape[0])
fake_y1 = np.ones(X_class1.shape[0])

# F-score (ANOVA) for each class separately
F0, _ = f_classif(X_class0, fake_y0)
F1, _ = f_classif(X_class1, fake_y1)

# Get indices of top 150 features for each class
top_150_class0 = np.argsort(F0)[-150:]
top_150_class1 = np.argsort(F1)[-150:]

# Combine and deduplicate
combined_indices = np.unique(np.concatenate([top_150_class0, top_150_class1]))
print(f" Selected {len(combined_indices)} unique features (balanced from both classes)")

# Subset the features using combined indices
X_selected = X.iloc[:, combined_indices]
X_selected['label'] = y.values

# Save to CSV
X_selected.to_csv('/content/test_features_balanced_top300.csv', index=False)
print(" Balanced top features saved to: /content/test_features_balanced_top300.csv")

import pandas as pd
import xgboost as xgb
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

# --------------------------
# Top 300 Features
# --------------------------
df_300 = pd.read_csv('test_features_balanced_top300.csv')
X_300 = df_300.drop('label', axis=1).values
y_300 = df_300['label'].values

# Impute missing values in y (labels) with the most frequent value (mode)
imputer_y_300 = SimpleImputer(strategy='most_frequent')
y_300_imputed = imputer_y_300.fit_transform(y_300.reshape(-1, 1)).flatten()

# Optional: Standardize
scaler_300 = StandardScaler()
X_300_scaled = scaler_300.fit_transform(X_300)

# XGBoost Classifier
xgb_300 = xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')
scores_300 = cross_val_score(xgb_300, X_300_scaled, y_300_imputed, cv=5, scoring='accuracy')
print(" Accuracy with top 300 features:", round(scores_300.mean(), 4))

# --------------------------
# All 2048 Features
# --------------------------
df_full = pd.read_csv('test_features.csv')
X_full = df_full.drop('label', axis=1).values
y_full = df_full['label'].values

# Impute missing values in y (labels) with the most frequent value (mode)
imputer_y_full = SimpleImputer(strategy='most_frequent')
y_full_imputed = imputer_y_full.fit_transform(y_full.reshape(-1, 1)).flatten()

# Standardize
scaler_full = StandardScaler()
X_full_scaled = scaler_full.fit_transform(X_full)

# XGBoost Classifier
xgb_full = xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')
scores_full = cross_val_score(xgb_full, X_full_scaled, y_full_imputed, cv=5, scoring='accuracy')
print(" Accuracy with all 2048 features:", round(scores_full.mean(), 4))

import pandas as pd
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, f1_score

# Load datasets
train_df = pd.read_csv("train_features_balanced_top300.csv")
test_df = pd.read_csv("test_features_balanced_top300.csv")

# Split features and labels
X_train = train_df.drop('label', axis=1)
y_train = train_df['label']

X_test = test_df.drop('label', axis=1)
y_test = test_df['label']

# Train SVM classifier with RBF kernel
svm = SVC(kernel='linear')
svm.fit(X_train, y_train)

# Predict on test data
y_pred = svm.predict(X_test)

# Evaluate
accuracy = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred, average='weighted')

print(f" Accuracy (linear Kernel): {accuracy * 100:.2f}%")
print(f" F1 Score (linear Kernel): {f1 * 100:.2f}%")

import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score

# Load CSVs
train_df = pd.read_csv("train_features_balanced_top300.csv")
test_df = pd.read_csv("test_features_balanced_top300.csv")

# Separate features and labels
X_train = train_df.drop('label', axis=1)
y_train = train_df['label']
X_test = test_df.drop('label', axis=1)
y_test = test_df['label']

# Normalize features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# PCA to reduce dimensions
pca = PCA(n_components=100)
X_train_pca = pca.fit_transform(X_train_scaled)
X_test_pca = pca.transform(X_test_scaled)

# Grid Search for SVM
param_grid = {
    'C': [0.1, 1, 10],
    'gamma': [1, 0.1, 0.01],
    'kernel': ['rbf']
}
grid_svm = GridSearchCV(SVC(probability=True), param_grid, refit=True, cv=5)
grid_svm.fit(X_train_pca, y_train)
best_svm = grid_svm.best_estimator_

# Ensemble Voting Classifier
lr = LogisticRegression(max_iter=1000)
rf = RandomForestClassifier(n_estimators=100)
gb = GradientBoostingClassifier(n_estimators=100)

voting_clf = VotingClassifier(estimators=[
    ('lr', lr),
    ('rf', rf),
    ('gb', gb),
    ('svm', best_svm)
], voting='soft')

# Train and Evaluate
voting_clf.fit(X_train_pca, y_train)
y_pred = voting_clf.predict(X_test_pca)

# Metrics
acc = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred, average='weighted')

print(" Accuracy:", acc)
print(" F1 Score:", f1)
print("ðŸ“Œ Best SVM Parameters:", grid_svm.best_params_)

import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.decomposition import PCA
from sklearn.pipeline import Pipeline
from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from xgboost import XGBClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, f1_score

# Load the data
train_df = pd.read_csv("train_features_balanced_top300.csv")
test_df = pd.read_csv("test_features_balanced_top300.csv")

X_train = train_df.drop("label", axis=1)
y_train = train_df["label"]
X_test = test_df.drop("label", axis=1)
y_test = test_df["label"]

# Preprocessing
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# PCA
pca = PCA(n_components=100)
X_train_pca = pca.fit_transform(X_train_scaled)
X_test_pca = pca.transform(X_test_scaled)

# Base learners
base_learners = [
    ("rf", RandomForestClassifier(n_estimators=100, random_state=42)),
    ("gb", GradientBoostingClassifier(n_estimators=100, random_state=42)),
    ("xgb", XGBClassifier(use_label_encoder=False, eval_metric="logloss", random_state=42)),
    ("svm", SVC(kernel='rbf', C=10, gamma=0.1, probability=True, random_state=42))
]

# Meta-learner
meta_learner = LogisticRegression(max_iter=1000)

# Stacking ensemble
stacking_model = StackingClassifier(estimators=base_learners, final_estimator=meta_learner, cv=5)

# Train the model
stacking_model.fit(X_train_pca, y_train)

# Predict and evaluate
y_pred = stacking_model.predict(X_test_pca)
accuracy = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred, average='weighted')

print(" Accuracy:", accuracy)
print(" F1 Score:", f1)


import shutil

source_path = '/content/train_features_balanced_top300.csv'
destination_path = '/content/drive/MyDrive/ml_proj/test_features.csv'

shutil.move(source_path, destination_path)

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from xgboost import XGBClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, f1_score, classification_report
import matplotlib.pyplot as plt
import seaborn as sns

# Load the data
train_df = pd.read_csv("train_features_balanced_top300.csv")
test_df = pd.read_csv("test_features_balanced_top300.csv")

X_train = train_df.drop("label", axis=1)
y_train = train_df["label"]
X_test = test_df.drop("label", axis=1)
y_test = test_df["label"]

# Preprocessing
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

models = {
    "Random Forest": RandomForestClassifier(n_estimators=200, max_depth=None, min_samples_split=2,
                                           min_samples_leaf=1, max_features='sqrt', random_state=42,
                                           n_jobs=-1),
    "Gradient Boosting": GradientBoostingClassifier(n_estimators=200, learning_rate=0.1,
                                                  max_depth=5, random_state=42),
    "XGBoost": XGBClassifier(n_estimators=200, learning_rate=0.1, max_depth=5,
                           use_label_encoder=False, eval_metric="logloss", random_state=42),
    "SVM": SVC(kernel='rbf', C=10, gamma='scale', probability=True, random_state=42)
}

results = {}

for name, model in models.items():
    print(f"\nTraining {name}...")
    model.fit(X_train_scaled, y_train)

    y_pred = model.predict(X_test_scaled)

    accuracy = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred, average='weighted')

    results[name] = {'accuracy': accuracy, 'f1': f1}

    print(f" {name} - Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}")
    print(classification_report(y_test, y_pred))

# Identify best performing model
best_model_name = max(results, key=lambda x: results[x]['accuracy'])
best_model_accuracy = results[best_model_name]['accuracy']

print(f"\n Best model: {best_model_name} with accuracy {best_model_accuracy:.4f}")

# Define smaller parameter grids and use RandomizedSearchCV instead of GridSearchCV
if best_model_name == "Random Forest":
    param_grid = {
        'n_estimators': [200, 300],
        'max_depth': [None, 10],
        'min_samples_split': [2, 5]
    }
    best_base = RandomForestClassifier(random_state=42, n_jobs=-1)

elif best_model_name == "Gradient Boosting":
    param_grid = {
        'n_estimators': [200],
        'learning_rate': [0.05, 0.1],
        'max_depth': [3, 5]
    }
    best_base = GradientBoostingClassifier(random_state=42)

elif best_model_name == "XGBoost":
    param_grid = {
        'n_estimators': [200],
        'learning_rate': [0.05, 0.1],
        'max_depth': [3, 5],
        'subsample': [0.8]
    }
    best_base = XGBClassifier(use_label_encoder=False, eval_metric="logloss", random_state=42)

else:  # SVM
    param_grid = {
        'C': [1, 10],
        'gamma': ['scale', 0.1],
        'kernel': ['rbf']
    }
    best_base = SVC(probability=True, random_state=42)

# Use RandomizedSearchCV instead of GridSearchCV for faster search
print(f"\nFine-tuning {best_model_name}...")
cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)  # Reduced folds from 5 to 3

# Use RandomizedSearchCV with fewer iterations
search = RandomizedSearchCV(
    estimator=best_base,
    param_distributions=param_grid,
    n_iter=5,  # Try only 5 combinations
    cv=cv,
    scoring='accuracy',
    n_jobs=-1,
    verbose=1,
    random_state=42
)

# Fit the model
search.fit(X_train_scaled, y_train)

# Get the best model
best_model = search.best_estimator_
print(f"Best parameters: {search.best_params_}")

# Evaluate best model
y_pred = best_model.predict(X_test_scaled)
final_accuracy = accuracy_score(y_test, y_pred)
final_f1 = f1_score(y_test, y_pred, average='weighted')

print(f"\n Final model - Accuracy: {final_accuracy:.4f}, F1 Score: {final_f1:.4f}")
print(classification_report(y_test, y_pred))

# Save the best model if accuracy improved
if final_accuracy > best_model_accuracy:
    # Plot feature importances if available
    if hasattr(best_model, 'feature_importances_'):
        feature_importance = best_model.feature_importances_
        feature_names = X_train.columns

        # Create DataFrame for visualization
        fi_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})
        fi_df = fi_df.sort_values(by='Importance', ascending=False)

        # Plot top 20 features
        plt.figure(figsize=(12, 8))
        sns.barplot(x='Importance', y='Feature', data=fi_df.head(20))
        plt.title(f'Top 20 Feature Importances - {best_model_name}')
        plt.tight_layout()
        plt.savefig('feature_importances.png')

        print("\nTop 10 important features:")
        print(fi_df.head(10))
